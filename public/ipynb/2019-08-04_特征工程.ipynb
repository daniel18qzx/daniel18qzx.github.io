{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 特征工程\n",
    "date: 2019-08-04\n",
    "categories: [人工智能, 数据挖掘]\n",
    "mathjax: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集载入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T09:57:21.505102Z",
     "start_time": "2019-08-09T09:57:19.805885Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv(u\"2019-08-01_金融数据描述_data1.csv\",encoding = 'gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征衍生/升维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征衍生是指用原始数据进行特征学习得到新的特征。衍生特征一般有两种原因引起的：数据自身的变化，使数据中出现很多原来没有的特征；进行特征学习时，算法根据特征之间的某种关系，产生了衍生特征，有时衍生特征更能反应数据特征之间的关系 。衍生特征也要求机器学习和深度学习算法拥有更强的学习能力，即增量学习、在线学习、迁移学习。\n",
    "\n",
    "衍生特征相对于原始特征能够更好的反映特征与数据的关系，因此对于某些数据来说这是极为重要的一个步骤。观察本数据，发现可以从衍生出以下几个特征，而如 latest_one_month_suc、latest_one_month_fail、latest_six_month_loan 都是处理好的特征："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 交易失败率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T09:59:29.647566Z",
     "start_time": "2019-08-09T09:59:29.641490Z"
    }
   },
   "outputs": [],
   "source": [
    "df['failurerate_last_1_month'] = df['trans_fail_top_count_enum_last_1_month'] / df['trans_top_time_last_1_month']\n",
    "df['failurerate_last_6_month'] = df['trans_fail_top_count_enum_last_6_month'] / df['trans_top_time_last_6_month']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 查询内容占比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T10:01:12.043408Z",
     "start_time": "2019-08-09T10:01:12.037814Z"
    }
   },
   "outputs": [],
   "source": [
    "df['query_finance_percent'] = df['query_finance_count'] / df['query_sum_count']\n",
    "df['query_cash_percent'] = df['query_cash_count'] / df['query_sum_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 每单平均交易量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T10:11:15.473082Z",
     "start_time": "2019-08-09T10:11:15.450821Z"
    }
   },
   "outputs": [],
   "source": [
    "df['per_avg_amount'] = df['historical_trans_amount'] / df['number_of_trans_from_2011']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 每天平均交易量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T10:11:16.284253Z",
     "start_time": "2019-08-09T10:11:16.278584Z"
    }
   },
   "outputs": [],
   "source": [
    "df['avg_amount_perday'] = df['historical_trans_amount'] / df['historical_trans_day']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征筛选/降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征选择( Feature Selection )也称特征子集选择( Feature Subset Selection , FSS )，或属性选择( Attribute Selection )。是指从已有的M个特征(Feature)中选择N个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程,是提高学习算法性能的一个重要手段,也是模式识别中关键的数据预处理步骤。对于一个学习算法来说,好的学习样本是训练模型的关键。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 共线性分析\n",
    "\n",
    "共线性问题指的是输入的自变量之间存在较高的线性相关度。共线性问题会导致回归模型的稳定性和准确性大大降低，另外，过多无关的维度计算也很浪费时间。\n",
    "\n",
    "变量出现共线性的原因：\n",
    "\n",
    "- 数据样本不够，导致共线性存在偶然性，这其实反映了缺少数据对于数据建模的影响，共线性仅仅是影响的一部分\n",
    "- 多个变量都给予时间有共同或相反的演变趋势，例如春节期间的网络销售量和销售额都相对与正常时间有下降趋势。\n",
    "- 多个变量存在一定的推移关系，但总体上变量间的趋势一致，只是发生的时间点不一致，例如广告费用和销售额之间，通常是品牌广告先进行大范围的曝光和信息推送，经过一定时间传播之后，才会在销售额上做出反映。\n",
    "- 多变量之间存在线性的关系。例如y代表访客数，用x代表展示广告费用，那么二者的关系很可能是y=2*x + b\n",
    "\n",
    "如何检验共线性：\n",
    "\n",
    "- 容忍度（Tolerance）：容忍度是每个自变量作为因变量对其他自变量进行回归建模时得到的残差比例，大小用1减得到的决定系数来表示。容忍度值越小说明这个自变量与其他自变量间越可能存在共线性问题。\n",
    "- 方差膨胀因子：VIF是容忍度的倒数，值越大则共线性问题越明显，通常以10作为判断边界。当VIF<10,不存在多重共线性；当10<=VIF<100,存在较强的多重共线性；当VIF>=100, 存在严重多重共线性。\n",
    "- 特征值（Eigenvalue）：该方法实际上就是对自变量做主成分分析，如果多个维度的特征值等于0，则可能有比较严重的共线性。\n",
    "- 相关系数：如果相关系数R>0.8时就可能存在较强相关性\n",
    "\n",
    "如何处理共线性：\n",
    "\n",
    "- 增大样本量：增大样本量可以消除犹豫数据量不足而出现的偶然的共线性现象，在可行的前提下这种方法是需要优先考虑的\n",
    "- 岭回归法（Ridge Regression）：实际上是一种改良最小二乘估计法。通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价来获得更实际和可靠性更强的回归系数。因此岭回归在存在较强共线性的回归应用中较为常用。\n",
    "- 逐步回归法（Stepwise Regression）:每次引入一个自变量进行统计检验，然后逐步引入其他变量，同时对所有变量的回归系数进行检验，如果原来引入的变量由于后面变量的引入而变得不再显著，那么久将其剔除，逐步得到最有回归方程。\n",
    "- 主成分回归（Principal Components Regression）:通过主成分分析，将原始参与建模的变量转换为少数几个主成分，么个主成分是原变量的线性组合，然后基于主成分做回归分析，这样也可以在不丢失重要数据特征的前提下避开共线性问题。\n",
    "- 人工去除：结合人工经验，对自变量进行删减，但是对操作者的业务能力、经验有很高的要求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T10:58:14.441499Z",
     "start_time": "2019-08-09T10:58:14.351553Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trans_activity_day                       historical_trans_day                       0.855832\n",
       "first_transaction_time                   first_transaction_day                     -0.985546\n",
       "historical_trans_amount                  per_avg_amount                             0.870366\n",
       "historical_trans_day                     trans_activity_day                         0.855832\n",
       "rank_trad_1_month                        top_trans_count_last_1_month               0.857462\n",
       "top_trans_count_last_1_month             rank_trad_1_month                          0.857462\n",
       "trans_top_time_last_1_month              consume_top_time_last_1_month              0.925764\n",
       "trans_top_time_last_6_month              consume_top_time_last_6_month              0.921039\n",
       "consume_top_time_last_1_month            trans_top_time_last_1_month                0.925764\n",
       "consume_top_time_last_6_month            trans_top_time_last_6_month                0.921039\n",
       "trans_fail_top_count_enum_last_6_month   trans_fail_top_count_enum_last_12_month    0.886050\n",
       "trans_fail_top_count_enum_last_12_month  trans_fail_top_count_enum_last_6_month     0.886050\n",
       "first_transaction_day                    first_transaction_time                    -0.985546\n",
       "apply_score                              loans_score                                0.973085\n",
       "query_org_count                          query_finance_count                        0.885501\n",
       "                                         query_cash_count                           0.846200\n",
       "                                         query_sum_count                            0.947786\n",
       "                                         latest_three_month_apply                   0.820085\n",
       "                                         latest_six_month_apply                     0.880014\n",
       "query_finance_count                      query_org_count                            0.885501\n",
       "                                         query_sum_count                            0.860472\n",
       "                                         latest_six_month_apply                     0.819415\n",
       "query_cash_count                         query_org_count                            0.846200\n",
       "query_sum_count                          query_org_count                            0.947786\n",
       "                                         query_finance_count                        0.860472\n",
       "                                         latest_three_month_apply                   0.875367\n",
       "                                         latest_six_month_apply                     0.941461\n",
       "latest_one_month_apply                   latest_three_month_apply                   0.869175\n",
       "                                         latest_six_month_apply                     0.804009\n",
       "latest_three_month_apply                 query_org_count                            0.820085\n",
       "                                                                                      ...   \n",
       "loans_cash_count                         loans_settle_count                         0.873677\n",
       "                                         loans_org_count_behavior                   0.943206\n",
       "                                         loans_product_count                        0.993300\n",
       "latest_six_month_loan                    loans_count                                0.890797\n",
       "                                         loans_settle_count                         0.845272\n",
       "                                         loans_org_count_behavior                   0.833786\n",
       "                                         history_suc_fee                            0.811367\n",
       "history_suc_fee                          loans_count                                0.914117\n",
       "                                         loans_settle_count                         0.932088\n",
       "                                         loans_org_count_behavior                   0.829458\n",
       "                                         latest_six_month_loan                      0.811367\n",
       "loans_credit_limit                       loans_avg_limit                            0.901755\n",
       "loans_org_count_current                  loans_count                                0.889557\n",
       "                                         loans_settle_count                         0.873677\n",
       "                                         loans_org_count_behavior                   0.943206\n",
       "                                         loans_product_count                        0.993300\n",
       "loans_product_count                      loans_count                                0.889684\n",
       "                                         loans_settle_count                         0.873214\n",
       "                                         loans_org_count_behavior                   0.937815\n",
       "                                         loans_cash_count                           0.993300\n",
       "                                         loans_org_count_current                    0.993300\n",
       "loans_avg_limit                          loans_credit_limit                         0.901755\n",
       "consfin_credit_limit                     consfin_avg_limit                          0.901470\n",
       "consfin_org_count_current                consfin_product_count                      0.988027\n",
       "consfin_product_count                    consfin_org_count_behavior                 0.988027\n",
       "                                         consfin_org_count_current                  0.988027\n",
       "consfin_avg_limit                        consfin_credit_limit                       0.901470\n",
       "per_avg_amount                           historical_trans_amount                    0.870366\n",
       "                                         avg_amount_perday                          0.850901\n",
       "avg_amount_perday                        per_avg_amount                             0.850901\n",
       "Length: 92, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = df.corr()\n",
    "corr_matrix = corr_matrix.unstack()\n",
    "corr_matrix[(abs(corr_matrix)>0.8) & (abs(corr_matrix) != 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 去掉方差较小的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方差阈值（VarianceThreshold）是特征选择的一个简单方法，去掉那些方差没有达到阈值的特征。默认情况下，删除零方差的特征，例如那些只有一个值的样本。假设我们有一个有布尔特征的数据集，然后我们想去掉那些超过80%的样本都是0（或者1）的特征。布尔特征是伯努利随机变量，方差为 p(1-p)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T11:36:30.818584Z",
     "start_time": "2019-08-09T11:36:30.806172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "#得到返回至少含有90%特征信息的特征\n",
    "sp = VarianceThreshold(threshold=0.8 * 0.2).fit(iris.data, iris.target)\n",
    "\n",
    "#可以看到哪些特征被保留\n",
    "X_result = sp.fit_transform(X, y)\n",
    "\n",
    "#输出结果\n",
    "sp.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单变量特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单变量的特征选择是通过基于单变量的统计测试来选择最好的特征，它可以当做是评估器的预处理步骤。\n",
    "\n",
    "Scikit-learn 将特征选择的内容作为实现了 transform 方法的对象\n",
    "\n",
    "1. SelectKBest移除那些除了评分最高的 K 个特征之外的所有特征\n",
    "2. SelectPercentile移除除了用户指定的最高得分百分比之外的所有特征\n",
    "\n",
    "这些对象将得分函数作为输入，返回单变量的得分和 p 值:\n",
    "\n",
    "对于回归: f_regression , mutual_info_regression\n",
    "对于分类: chi2 , f_classif , mutual_info_classif 可自行查看官网API文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T11:53:10.721567Z",
     "start_time": "2019-08-09T11:53:10.704548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>filter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SepalLengthCm</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SepalWidthCm</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PetalLengthCm</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PetalWidthCm</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         columns  filter\n",
       "0  SepalLengthCm    True\n",
       "1   SepalWidthCm   False\n",
       "2  PetalLengthCm    True\n",
       "3   PetalWidthCm    True"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "iris = load_iris()\n",
    "iris.columns = ['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "#得到返回至少含有90%特征信息的特征\n",
    "sp = SelectPercentile(f_classif, percentile= 90)\n",
    "\n",
    "#可以看到哪些特征被保留\n",
    "X_result = sp.fit_transform(X, y)\n",
    "\n",
    "#输出结果\n",
    "sp.get_support()\n",
    "\n",
    "pd.DataFrame({'columns':iris.columns,'filter':sp.get_support()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于 L1 的特征选取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T11:53:28.736322Z",
     "start_time": "2019-08-09T11:53:28.720287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>filter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SepalLengthCm</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SepalWidthCm</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PetalLengthCm</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PetalWidthCm</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         columns  filter\n",
       "0  SepalLengthCm    True\n",
       "1   SepalWidthCm    True\n",
       "2  PetalLengthCm    True\n",
       "3   PetalWidthCm   False"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "iris = load_iris()\n",
    "iris.columns = ['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "sp = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = sp.transform(X)\n",
    "\n",
    "#输出结果\n",
    "sp.get_support()\n",
    "\n",
    "pd.DataFrame({'columns':iris.columns,'filter':sp.get_support()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于决策树的特征选取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T11:53:56.940887Z",
     "start_time": "2019-08-09T11:53:56.927253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>filter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SepalLengthCm</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SepalWidthCm</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PetalLengthCm</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PetalWidthCm</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         columns  filter\n",
       "0  SepalLengthCm   False\n",
       "1   SepalWidthCm   False\n",
       "2  PetalLengthCm   False\n",
       "3   PetalWidthCm    True"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "iris = load_iris()\n",
    "iris.columns = ['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "forest = DecisionTreeClassifier(criterion='entropy').fit(X, y)\n",
    "sp = SelectFromModel(forest, prefit=True)\n",
    "X_new = sp.transform(X)\n",
    "\n",
    "#输出结果\n",
    "sp.get_support()\n",
    "\n",
    "pd.DataFrame({'columns':iris.columns,'filter':sp.get_support()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T11:53:57.315831Z",
     "start_time": "2019-08-09T11:53:57.304330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>filter</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PetalLengthCm</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PetalWidthCm</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SepalWidthCm</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SepalLengthCm</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         columns  filter  importance\n",
       "2  PetalLengthCm   False           0\n",
       "3   PetalWidthCm    True           1\n",
       "1   SepalWidthCm   False           2\n",
       "0  SepalLengthCm   False           3"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#输出特征排序\n",
    "importance = forest.feature_importances_\n",
    "imp_result = np.argsort(importance)[::-1]\n",
    "\n",
    "pd.DataFrame({'columns':iris.columns,'filter':sp.get_support(),'importance':imp_result}).sort_values(\"importance\",ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WOE\n",
    "\n",
    "全称是“Weight of Evidence”，即证据权重。WOE是对原始自变量的一种编码形式。\n",
    "要对一个变量进行WOE编码，需要首先把这个变量进行分组处理（也叫离散化、分箱等等，说的都是一个意思）。分组后，对于第i组，WOE的计算公式如下：\n",
    "\n",
    "其中，pyi是这个组中响应客户（风险模型中，对应的是违约客户，总之，指的是模型中预测变量取值为“是”或者说1的个体）占所有样本中所有响应客户的比例，pni是这个组中未响应客户占样本中所有未响应客户的比例，#yi是这个组中响应客户的数量，#ni是这个组中未响应客户的数量，#yT是样本中所有响应客户的数量，#nT是样本中所有未响应客户的数量。\n",
    "\n",
    "从这个公式中我们可以体会到，WOE表示的实际上是“当前分组中响应客户占所有响应客户的比例”和“当前分组中没有响应的客户占所有没有响应的客户的比例”的差异。\n",
    "\n",
    "对这个公式做一个简单变换，可以得到：\n",
    "\n",
    "变换以后我们可以看出，WOE也可以这么理解，他表示的是当前这个组中响应的客户和未响应客户的比值，和所有样本中这个比值的差异。这个差异是用这两个比值的比值，再取对数来表示的。WOE越大，这种差异越大，这个分组里的样本响应的可能性就越大，WOE越小，差异越小，这个分组里的样本响应的可能性就越小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 参考：\n",
    "\n",
    "1. [DataWhale数据挖掘实战营](https://github.com/datawhalechina/Datawhale_Learning/tree/master/doc/%E7%90%86%E8%AE%BA%E5%BA%94%E7%94%A8/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98)\n",
    "2. [python数据预处理 ：数据共线性处理](https://blog.csdn.net/tonydz0523/article/details/84404261)\n",
    "3. [特征选择 (feature_selection)](https://www.cnblogs.com/stevenlk/p/6543628.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
