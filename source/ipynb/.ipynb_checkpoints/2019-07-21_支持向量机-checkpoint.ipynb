{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 支持向量机\n",
    "date: 2019-07-21\n",
    "categories: [人工智能, 监督学习]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 能适应\"样本数少，特征维度高\"的数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基本原则\n",
    "1. 学习数据空间中的一个超平面达到二值分类目的，在普通线性可分问题中，符合分类要求的超平面会有无穷多个\n",
    "2. 尽可能找到最能平均分配两类数据中空白区域的超平面，亦即使得\"与超平面距离最小的数据点的距离\"最大化\n",
    "    - 平行于最优超平面(Optional Hyperplane)，且含括两类数据中最靠近超平面的点(此两数据点即为Support Vector)的超平面距离应越大越好\n",
    "    - 最优超平面到上述两超平面的距离应相等(此处距离即为间隔Margin)\n",
    "\n",
    "#### 当数据出现噪声\n",
    "1. 当数据出现噪声，Margin过低时会导致模型过拟合，此时应加入\"软间隔(Soft Margin)\"概念\n",
    "    - 要尽可能正确地分类训练数据(松弛因子超参数C大)\n",
    "    - Margin 要尽可能大(松弛因子超参数C小)\n",
    "    \n",
    "#### 当数据在现有维度中找不到最优超平面\n",
    "1. 任何有限维度的非线性问题在更高维度的空间里总可以变化成线性可分问题\n",
    "2. SVM使用拉格朗日乘子法(Lagrange Multiplier)实现对超平面求解问题的升维，不需真的升维，而是将求超平面参数的目标转换为用高维中数据点向量两两点积值求解的二次规划问题\n",
    "3. 核函数(Kernel Function)就是一种输入两个低维空间向量，返回高维空间点积的函数。一些常用的核函数如下：\n",
    "    - 线性核(Linear)：直接返回输入向量的点积，速度最快，因为实际上没有升维，适用于本身特征维度较高、样本数量很大的场景\n",
    "    - 多项式核(Polynomial)：其中超参数d是提升到的维度\n",
    "    $$k(p,q)=(p*q+1)*d$$\n",
    "    - 高斯迳向基核(Gaussian Radial Basis Function)：应用最广泛，gamma值越大越容易过拟合\n",
    "    $$k(p,q)=exp(-\\gamma*||p-q||^2)$$\n",
    "    - Sigmoid核：\n",
    "    $$k(p,q)=tanh(a*p\\cdot q+r)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1]\n",
      "[ 0.52444566 -0.52444566]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "x = [[0,0],[2,2]]\n",
    "y = [1,2]\n",
    "\n",
    "clf = svm.SVC(kernel = \"rbf\")\n",
    "clf.fit(x,y)\n",
    "\n",
    "t = [[2,1],[0,1]]\n",
    "print(clf.predict(t))\n",
    "print(clf.decision_function(t))  # 返回数据到超平面的距离，越大则分类可靠性越高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 参考：\n",
    "\n",
    "1. [从机器学习到深度学习：基于Scikit-learn与TensorFlow的高效开发实战](http://www.broadview.com.cn/book/5337)\n",
    "2. [SVM 原理详解，通俗易懂](https://blog.csdn.net/DP323/article/details/80535863)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
