---
title: 梯度下降
categories:
  - 人工智能
  - 基础概念
abbrlink: 45545
date: 2019-07-20 00:00:00
---

## 步骤

1. 任取一点作为起始点
2. 查看在当前节点向哪个方向移动能得到最小的z值，并向该方向移动
3. 重复步骤直到找不到更小的z值

## SGD的出现

计算普通梯度下降时，每次都必须遍历所有样本，计算资源过大。SGD是每次采用单一或小批量样本的方差和作为损失值。

- 迭代次数虽变多，但每次迭代速度快，整体还是比较有效率达到最适点
- 由于数据存在噪声，不代表每次迭代都是正确的，但整体还是往最适点方向前进
- 由于是随机选取样本，有可能跨过local minimum，去到global minimum

## SGDRegressor


```python
from sklearn.linear_model import SGDRegressor

x = [[0,0],[2,1],[5,4]]
y = [0,2,2]

reg = SGDRegressor(penalty = "l2", max_iter = 10000)
# penalty: none, l1(Lasso,绝对值), l2(Ridge,平方), elasticent
# max_iter防止连续抖动
reg.fit(x,y)

print(reg.predict([[4,3]]))
print(reg.coef_)
print(reg.intercept_)
```

    [1.97845294]
    [ 1.56550035 -1.47798393]
    [0.15040332]


## SGDClassifier


```python
clf = SGDRegressor(penalty = "l2", max_iter = 10000)
clf.fit(x,y)

print(clf.predict([[4,3]]))
```

    [1.97839971]


> 参考：

1. [从机器学习到深度学习：基于Scikit-learn与TensorFlow的高效开发实战](http://www.broadview.com.cn/book/5337)
