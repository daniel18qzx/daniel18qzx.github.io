---
title: 决策树
categories:
  - 人工智能
  - 监督学习
mathjax: true
abbrlink: 51507
date: 2019-07-29 00:00:00
---

## 信息价值指标

### 信息熵

$$H=-\sum_{i=1}^{n}{P_i \cdot log_2{P_i}}$$

- 某随机事件结果的种类越多，则该事件的熵越大（不确定性越大）
- 某随机事件的各种可能发生结果概率越均匀，则该事件的熵越大（不确定性越大）
- 值域：>= 0
- 缺点是计算量大（log）

### 基尼指数

$$G=1-\sum_{i=1}^{n}{P_i^2}$$

- 值域：0~1

## 建树逻辑

所有子树信息价值（信息熵、基尼指数）的和必定小于等于原来整体数据集的信息价值，信息增益用来衡量减少的程度。


> def build(D=数据集)：
>
> 　　if D所有数据目标值 y 都相同：
>
> 　　　　return  #递回停止条件
>
> 　　for i in D中的所有特征：
>
> 　　　　计算用i划分子树后获得的信息增益
>
> 　　if 所有特征都没有大于零的信息增益：
>
> 　　　　return  #递回停止条件
>
> 　　被选择的特征 x = 具有最大信息增益的特征
>
> 　　for sub in 按照x划分子树后的数据集：
>
> 　　　　build(sub)


## 衍生算法

- ID3：只能使用熵的信息增益处理离散特征的分类问题
- C4.5：
    - 使用信息增益比的概念去除先选择多值特征的倾向
    - 支持连续特征，在计算信息增益比之前首先将其离散化
    - 在训练后检测训练集的正确分类比，并剪枝产生错误较多的叶子结点
- CART算法：使用基尼系数代替熵进行信息增益计算、只使用二叉树，并提供解决回归问题的能力

## DecisionTreeClassifier

超参数：
- criterion: gini, entropy
- max_depth
- min_samples_split
- min_samples_leaf
- max_leaf_nodes
- min_impurity_decrease


```python
from sklearn import tree

X = [[2000, 30, 400], [370, 130, 0], [3020, 120, 300]]
Y = [1, 0, 1]

clf = tree.DecisionTreeClassifier(criterion = "entropy")
clf = clf.fit(X, Y)

print(clf.predict([[4000, 6000, 0]]))
print(clf.feature_importances_)
```

    [0]
    [0. 0. 1.]


## 树的可视化


```python
from IPython.display import display, Image
import graphviz
import pydotplus as pdp

dot_data = tree.export_graphviz(clf, out_file = None,
                               feature_names = ["标签A","标签B","标签C"],
                               class_names = ["分类A","分类B"],
                               filled = True,
                               rotate = False)

# 要先用brew install graphviz
graph = pdp.graph_from_dot_data(dot_data)
img = Image(graph.create_png())
```

![](/img/2019-07-29_决策树_1.png)

> 参考：

1. [从机器学习到深度学习：基于Scikit-learn与TensorFlow的高效开发实战](http://www.broadview.com.cn/book/5337)
